---
title: 'mlr3 Imputation: An Introduction'
author: "Natalie Foss"
date: "2022-09-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction {#sec-introduction}

This post is a part of a series about `mlr3`, see the other posts:

-   Create your first R Project: [link](https://nfoss2.github.io/mlr3_first_project_post)

-   Learn the right way to train simple models: [link](https://nfoss2.github.io/mlr3_models_post)

-   Learn how to impute missing values with pipelines (this post)

The purpose of this post is to introduce you to `mlr3` imputation and give an example of how to impute missing values using pipelines (showing the best practice methods), using the `palmer penguins` dataset. `palmer penguins` is a built in task described as "classification data to predict the species of penguins". Read more about the dataset [here](https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html).

## Set Up the Environment {#sec-set-up-the-environment}

To follow along with this post you will to run the following code chunk:

```{r}
set.seed(78)
# install.packages("mlr3")
library("mlr3")

# create task object
task = tsk("penguins")

# create a resampling object
cv = rsmp("cv", folds=10)

# create a performance measure object
measure = msr("classif.ce")
```

## Missing Values {#sec-missing-values}

Many data sets have missing values, this can cause issues with some learners. To see if your data set has missing values use `$missings()`

```{r}
task$missings()
```

## What is Imputation? {#sec-what-is-imputation}

**Imputation** is a method of identifying and handling (removing or replacing) missing values in a data set.

There are many different kinds of imputation, most have `mlr3` implementations. Short explanations of each are in this table.

| Imputation Method | Description                                                                                                                                                            | Parameters                    | Link to Docs                                                                                   |
|-------------|--------------------------------|-------------|----------------|
| "imputeconstant"  | Replaces all missing values with a constant.                                                                                                                           | `constant`                    | Read more [here](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputeconstant.html). |
| "imputehist"      | Replaces all missing values from a histogram.                                                                                                                          |                               | Read more [here](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html).     |
| "imputelearner"   | Replaces all missing values by fitting a learner.                                                                                                                      | `learner`                     | Read more [here](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputelearner.html).  |
| "imputemean"      | Replaces all numerical missing values with the mean or the column.                                                                                                     |                               | Read more [here](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputemean.html).     |
| "imputemedian"    | Replaces all numerical missing values with the median or the column.                                                                                                   |                               | Read more [here](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputemedian.html).   |
| "imputemode"      | Replaces all numerical missing values with the mode or the column.                                                                                                     |                               | Read more [here](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputemode.html).     |
| "imputeoor"       | Replaces all numerical missing values with constant values shifted below the minimum or above the maximum. This method is useful in the context of tree-based methods. | `min`, `offset`, `multiplier` | Read more [here](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputeoor.html).      |
| "imputesample"    | Replaces all numerical missing values by sampling from the non-missing training data.                                                                                  |                               | Read more [here](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputesample.html).   |

: Imputation methods

## Create Pipeline {#sec-create-pipeline}

```{r}
# install.packages("mlr3pipelines")
library("mlr3pipelines")
# install.packages("e1071") # for `classif.naive_bayes`
library(e1071)
# install.packages("mlr3learners")
library(mlr3learners)
```

For this post we will use a pipeline to automate the process, read about pipelines [here](https://mlr3book.mlr-org.com/pipelines.html). We will start by creating the necessary `pipeOps`.

1.  An Imputation method - for this example we will use `imputesample` (impute features by sampling).

2.  Two learners

    a\. `classif.rpart` - This learner can accomadate with missing values, see an example of its usage in this [post](https://nfoss2.github.io/mlr3_models_post).

    b\. `classif.naive_bayes` - This learner cannot accommodate missing values. If the missing data in `penguins` was not imputed we would not be able to use this learner.

```{r}
# imputation method
poImpute = po("imputesample")

# learners
poRPLrn = po("learner", learner = lrn("classif.rpart"), id="rpart")
poNBLrn = po("learner", learner = lrn("classif.naive_bayes"), id="n_b")
```

Next we will create the graph that will impute features then train the learner. We will create a branch to specify to two different paths (no imputation -\> `classif.rpart` learner and imputation -\> `classif.naive_bayes` learner). The last step of the graph is to "unbranch". Then we will be able to compare the performance of each path by specifying the branch to run.

```{r}
# branch options
options = c("nothing", "impute")

graph = po("branch", options) %>>% 
  gunion(list(
    poRPLrn, 
    poImpute %>>% poNBLrn)) %>>%
  po("unbranch", options)

graph$plot()
```

## Test imputation vs. No Imputation

We will specify that we want to graph to run the no imputation branch "nothing".

```{r}
graph$param_set$values$branch.selection = "nothing"
```

The next step is to make the graph into a graph learner so that we can use a resampling method.

```{r}
grlrn = as_learner(graph)
```

Now for the training step:

```{r}
rr = resample(task, grlrn, cv)
```

Now that the training is complete we can see the performance of the model.

```{r}
rr$aggregate(measure)
```

We can repeat these steps after specifying that we want to graph to run the imputation branch "impute".

```{r}
graph$param_set$values$branch.selection = "impute"
grlrn = as_learner(graph)
rr = resample(task, grlrn, cv)
rr$aggregate(measure)
```

We can see that in this case the impute branch had better performance (0.0204 classification error vs. 0.0553 classification error). Many learners cannot accommodate missing values (see [this list](https://mlr-org.com/learners.html) and look for `missings` in properties for those that can) so knowing how to impute these missing values can allow you to explore more models and maybe even get better performance.
