---
title: 'mlr3 Learners: An Introduction'
author: "Natalie Foss"
date: "2022-09-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The purpose of this post is to introduce you to `mlr3` learners and give an example of how to train a model (showing the best practice methods), using the `palmer penguins` dataset. `palmer penguins` is a built in task described as "classification data to predict the species of penguins". Read more about the dataset [here](https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html).

This blog post picks up from the previous [post](?) If you do not wish to read that post, this code chunk will get you up to speed (though we recommend you skim it).

```{r}
library("mlr3")
task = tsk("penguins")
learner = lrn("classif.rpart")
```

## Train and Test Split

As discussed in the last [post](?), when training a model and evaluating, it is important to consider data leakage. Data leakage occurs when a model is tested on some of all of the same data it was trained on. It results in overly optimistic performance results. For this reason we subset the data into train and test splits.

```{r}
# setting a seed will ensure your output matches the post output!
set.seed(3)
```

We will create two variables:

-   splits\$train - randomly samples 80% of the data points (rows) in the task (meaning it will hold the row indexes for 80% of the rows)
-   splits\$test - holds the remaining 20% of the row ids

`partition()` is a function that allows us to create balanced subsets of our data. we specify `cat_col` as "species" so that `partition()` balances the factors in each split. Meaning that the ratio of each species is the same in the train and test set. Read more about `partition()` [here](https://www.rdocumentation.org/packages/groupdata2/versions/1.1.2/topics/partition).

```{r}
splits = partition(task, ratio = 0.8, cat_col="species")
print(str(splits))
```

Now that we have data set aside for testing we can actually train the model.

## Training the Model

```{r}
learner$train(task, splits$train)
learner$model
```

Now we have this model (decision tree) that was trained on 80% of the data and we can now see it make some predictions on the test set.

```{r}
prediction = learner$predict(task, splits$test)
```

## Model Visualization

We can visualize the predictions with `autoplot()`:

```{r}
#install.packages("mlr3viz")
library("mlr3viz")

autoplot(prediction)
```

## Model Performance

Since this is a classification problem we can get the confusion matrix which will give us an idea of how well the model performed. The confusion matrix shows how many data points were correctly and incorrectly classified. The diagonal values denote the correct classifications and all other values denote incorrect classifications. Read more about confusion matrices [here](https://machinelearningmastery.com/confusion-matrix-machine-learning/).

```{r}
prediction$confusion
```

While the confusion matrix shows the model's performance generally, we can quantify the predictive performance of our model by calling `prediction$score()`. There are several different scoring methods to choose from (filtering here by classification measures).

```{r}
as.data.table(mlr_measures$keys("classif"))
```

Note: to see other types of learners replace "classif" with one of: "clust", "dens", "regr", or "surv".

For this example we will use classification error, read more about this measure in the [documentation](https://mlr3.mlr-org.com/reference/mlr_measures_classif.ce.html).

```{r}
measure = msr("classif.ce")
prediction$score(measure)
```

## Why Resampling is Important

Compare the performance of the previous model to this one, which is identically prepared except for a different seed for randomness.

```{r}
set.seed(22)
splits = partition(task, ratio = 0.8, cat_col="species")
learner$train(task, splits$train)
prediction = learner$predict(task, splits$test)
prediction$score(measure)
```

This is a massive difference in classification error. In the first model roughly 10% of the test set was incorrectly classified and in the second model only \~1.5% of the test set was incorrectly classified. Now we have no idea which result is an accurate representation of the model performance on this task. The solution to this problem is resampling.

Read about how to use resampling methods in `mlr3` in the next [post](?).
